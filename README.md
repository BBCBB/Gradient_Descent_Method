# Gradient_Descent_Method
This method is a numerical approach used for optimization in unconstrained convex programming, providing a sequence of solutions until it cannot make further significant improvements. Hence, we need to determine termination criteria in advance. It is needed to define an initial guess and beginning solution, which apparently must be in the domain of the cost function. The other assumption that must be made to use this method is that the cost function's sublevel sets must be closed to ensure that the solution sequence would not fall out of the domain.
In order to define the step length to move in the gradient direction, we should get into the line search process. If you want to define the best step length by an exact approach, you need to solve another constrained optimization problem. However, there are other inexact approaches that can perform very well, and one of them is Armijo Line Search. 

**Stochastic Gradient, Subgrdient, and Projected Gradient Methods will be added to this repo.**
